# Kaggle Competition Strategy Overview

| Category                                  | Technique or Feature                                  | Description                                                                                                                                                                                                                                                                                                                                                                                           |
|-------------------------------------------|-------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| New Feature Ideas                         | Lagged Returns and Volatility                         | Compute percentage change in closing price over one, three, five, ten, and fifteen minute intervals. Also calculate rolling standard deviation of those returns over five, fifteen, and thirty minute windows to capture short‑term volatility regimes. Include the Average True Range normalized to per‑minute bars to reflect average trading range and intrabar volatility.                                         |
| New Feature Ideas                         | Technical Indicators                                  | Add fourteen‑period Relative Strength Index to measure overbought/oversold momentum. Include stochastic oscillator %K and %D lines for trend reversal signals. Compute Exponential Moving Averages over five, fifteen, and sixty minute windows and their pairwise differences to capture cross‑over trend shifts. Calculate Bollinger Band width and %B (position within bands) to detect mean‑reversion opportunities.    |
| New Feature Ideas                         | Microstructure Proxies                                | Derive bid‑ask volume imbalance by aggregating traded volume on the bid side versus ask side in rolling five‑second buckets. Count buyer‑initiated versus seller‑initiated trades each minute to track order flow pressure. Compute Volume Weighted Average Price (VWAP) each minute and its deviation from the last closing price to identify execution costs and mean‑reversion signals.                            |
| New Feature Ideas                         | Cross Asset Signals                                   | Include Bitcoin returns over the previous one and five minute intervals—BTC moves often lead altcoin responses. Incorporate stablecoin funding rate or perpetual swap funding premium lagged by one to five minutes to capture leveraged market sentiment and potential mean‑reversion impulses when funding costs spike or invert.                                                                                   |
| New Feature Ideas                         | Time and Calendar                                     | Encode minute of day using sine and cosine transforms to capture daily intraday seasonality patterns. One‑hot encode each hour of the day for models that handle sparse inputs. Add a binary weekend flag to adjust for reduced liquidity and different trading dynamics on Saturdays and Sundays.                                                                                                          |
| New Feature Ideas                         | Lightweight Sentiment Proxy                           | Fetch rolling counts of Twitter mentions or Google Trends search volume for the asset, lagged by five to ten minutes, to approximate retail sentiment surges. Normalize by historical average to detect unusual spikes in social attention that often precede volatility jumps.                                                                                                                                      |
| Model and Architecture Tweaks             | Temporal Convolutional Network                        | Replace or augment LSTM with a TCN using causal dilated convolutional layers to capture dependencies across long sequences without recurrence. This architecture scales linearly with sequence length, reduces gradient vanishing, and delivers faster inference, making it suitable for high‑frequency real‑time prediction.                                                                                     |
| Model and Architecture Tweaks             | Attention Augmented Recurrent Neural Network          | Add a self‑attention mechanism (e.g., Bahdanau or Transformer‑style) on top of bi‑LSTM outputs so the model can dynamically weight key timesteps. This allows the network to focus on the most predictive past windows rather than relying solely on hidden‑state propagation, improving both interpretability and performance on non‑stationary time series.                                              |
| Model and Architecture Tweaks             | Hybrid Convolutional Neural Network plus Long Short Term Memory | Preprocess raw feature windows with a stack of one‑dimensional convolutional layers to automatically extract local temporal patterns and motifs. Feed the resulting feature maps into a bi‑LSTM to model sequential dependencies. This hybrid approach captures both short‑term microstructure patterns and longer trends with fewer parameters than deep recurrent stacks.                                              |
| Model and Architecture Tweaks             | Lightweight Transformer Encoder                       | Implement a compact Transformer encoder with two to three attention heads and two layers of self‑attention/ feed‑forward sublayers. Operate on one to two hours of aggregated features to learn global context and inter‑feature relationships. Prune or distill the model to maintain sub‑10 ms inference latency while benefiting from Transformer expressivity.                                            |
| Model and Architecture Tweaks             | Ensemble or Stacking                                  | Train a bi‑LSTM and a LightGBM model on the same enriched feature set. Combine their predicted probabilities through a logistic regression meta‑learner, which learns optimal blending weights. This ensemble balances the LSTM’s sequential modeling strength with the tree model’s ability to capture nonlinear feature interactions and reduces overall variance.                                    |
| Training and Deployment Improvements      | Label Smoothing and Thresholding                      | Introduce a small positive threshold (e.g., 0.01% return) for classifying an “up” move to filter out noise and reduce label ambiguity. Optionally add a third “flat” class for returns within ±threshold to focus the binary model on clearer directional moves and improve precision.                                                                                                                        |
| Training and Deployment Improvements      | Class Balancing and Loss Weighting                    | Apply focal loss or set class weights inversely proportional to class frequencies in the cross‑entropy loss function. This forces the model to pay more attention to the less frequent “up” instances, improving recall on price increases without sacrificing overall accuracy.                                                                                                                             |
| Training and Deployment Improvements      | Walk Forward Online Training                          | Use a rolling window retraining strategy: every hour or day, refit or fine‑tune the model on the most recent 24 to 48 hours of data using previously optimized hyperparameters from Optuna. This continuous update adapts to regime shifts, preserves model freshness, and prevents performance degradation over time.                                                                                    |
| Training and Deployment Improvements      | Feature Selection and Regularization                  | Use SHAP values from a proxy tree model to identify and drop low‑importance features, simplifying the input space. In LSTM layers, apply L₂ weight decay and dropout with probability 0.2 to mitigate overfitting to micro‑noise and improve generalization on unseen market conditions.                                                                                                                   |
| Training and Deployment Improvements      | Latency Aware Serving                                 | Quantize the trained model to 16‑bit floating point or integer precision to reduce memory footprint and accelerate inference. Serve with a lightweight FastAPI and Uvicorn setup, optimized for sub‑5 ms prediction per bar, ensuring the system meets real‑time trading latency requirements.                                                                                                     |
